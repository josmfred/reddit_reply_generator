{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install praw\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random submission from a given subreddit\n",
    "def get_random_submission(subreddit_name, reddit):\n",
    "    post = reddit.subreddit(subreddit_name).random()\n",
    "    return post\n",
    "\n",
    "# returns the text and the sentiment for each comment and\n",
    "# for each comment, if the comment has a parent returns\n",
    "# the parent and the comment as a child\n",
    "def get_comments_and_parents(post):\n",
    "    post.comments.replace_more(limit=None)\n",
    "    comments = post.comments.list()\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    parents = []\n",
    "    parents_scores = []\n",
    "    rtn_comments = []\n",
    "    scores = []\n",
    "    for comment in comments:\n",
    "        comment_parent = comment.parent()\n",
    "        comment_scores = vader_analyzer.polarity_scores(comment.body)\n",
    "        comment_scores_lst = [comment_scores[\"neg\"], comment_scores[\"neu\"],\n",
    "                              comment_scores[\"pos\"], comment_scores[\"compound\"]]\n",
    "        scores += [comment_scores_lst]\n",
    "        try:\n",
    "            parents += [comment_parent.body]\n",
    "            parent_scores = vader_analyzer.polarity_scores(comment_parent.body)\n",
    "            parent_scores_lst = [parent_scores[\"neg\"], parent_scores[\"neu\"],\n",
    "                                parent_scores[\"pos\"], parent_scores[\"compound\"]]\n",
    "            parents_scores += [parent_scores_lst]            \n",
    "            rtn_comments += [comment.body]\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return ([comment.body for comment in comments], scores,\n",
    "            [comment.score for comment in comments]),(parents, parents_scores, rtn_comments)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"k7TYWsSn5jpn_w\",\n",
    "                     client_secret = \"_X5Zeopsz9aSUTq_iMIRaDoYmv8\",\n",
    "                     user_agent='Python: Comment Scraper: v0.1(by /u/josmfred)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_texts, comment_sentiment_scores, comment_upvotes = [], [], []\n",
    "parent_texts, parent_sentiment_scores, children_texts = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (100):\n",
    "    score_predict_data, word_predict_data = (\n",
    "            get_comments_and_parents(get_random_submission(\"ProgrammerHumor\", reddit))\n",
    "    )\n",
    "    comment_texts.extend(score_predict_data[0])\n",
    "    comment_sentiment_scores.extend(score_predict_data[1])\n",
    "    comment_upvotes.extend(score_predict_data[2])\n",
    "    parent_texts.extend(word_predict_data[0])\n",
    "    parent_sentiment_scores.extend(word_predict_data[1])\n",
    "    children_texts.extend(word_predict_data[2])\n",
    "    if i % 9 == 0:\n",
    "        print(len(comment_texts))\n",
    "        pickle.dump(comment_texts, open(\"comment_texts.pkl\", \"wb\"))\n",
    "        pickle.dump(comment_sentiment_scores, open(\"comment_sentiment_scores.pkl\", \"wb\"))\n",
    "        pickle.dump(comment_upvotes, open(\"comment_upvotes.pkl\", \"wb\"))\n",
    "        pickle.dump(parent_texts, open(\"parent_texts.pkl\", \"wb\"))\n",
    "        pickle.dump(parent_sentiment_scores, open(\"parents_sentiment_scores.pkl\", \"wb\"))\n",
    "        pickle.dump(children_texts, open(\"children_texts.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having finished aquiring and saving the data, we will begin cleaning the data under the assumption that the data is saved in the current directory. The following portion of the notebook does not require any of the previous cells to have been run. All they require is that the correct data files are saved in the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_texts = pickle.load(open(\"comment_texts.pkl\", \"rb\"))\n",
    "sentiment_scores = pickle.load(open(\"comment_sentiment_scores.pkl\", \"rb\"))\n",
    "comment_upvotes = pickle.load(open(\"comment_upvotes.pkl\", \"rb\"))\n",
    "parent_texts = pickle.load(open(\"parent_texts.pkl\", \"rb\"))\n",
    "parent_sentiment_scores = pickle.load(open(\"parents_sentiment_scores.pkl\", \"rb\"))\n",
    "children_texts = pickle.load(open(\"children_texts.pkl\", \"rb\"))\n",
    "# The tokenizer might not exist. If the tokenizer does not exist, then\n",
    "# we assume that the data we are processing should have be used\n",
    "# to fit a new tokenizer, and then this tokenizer is saved to use in later\n",
    "# data processing. If the tokenizer does exist, we use the existing\n",
    "# tokenizer on the new data.\n",
    "try:\n",
    "    tokenizer = pickle.load(open(\"tokenizer\", \"rb\"))\n",
    "except:\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes and pads the comment data and vectorizes the upvote score data.\n",
    "def prepare_score_data(comments, scores, labels, max_comment_length, vocab_size=10000, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        tokenizer.fit_on_texts(comments)\n",
    "    token_comments = tokenizer.texts_to_sequences(comments)\n",
    "    pad_comments = sequence.pad_sequences(token_comments, maxlen=max_comment_length)\n",
    "    return pad_comments, np.array(scores), np.array(labels), tokenizer\n",
    "\n",
    "# tokenizes and pads the parent data and creates a array of the\n",
    "# first word in the child comment\n",
    "def prepare_next_word_data(parents, parents_scores, comments, texts,\n",
    "                           vocab_size=10000, max_comment_length=150, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "    token_parents = tokenizer.texts_to_sequences(parents)\n",
    "    token_child = tokenizer.texts_to_sequences(comments)\n",
    "    pad_parents = sequence.pad_sequences(token_parents, maxlen=max_comment_length)\n",
    "    labels = []\n",
    "    for child in token_child:\n",
    "        if len(child) >= 1:\n",
    "            labels.append([child[0]])\n",
    "        else:\n",
    "            labels.append([0])\n",
    "    return pad_parents, np.array(parents_scores), np.array(labels), tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_parents, parent_scores, first_word, tokenizer =  prepare_next_word_data(parent_texts,\n",
    "                                                                            parent_sentiment_scores,\n",
    "                                                                            children_texts,\n",
    "                                                                            comment_texts,\n",
    "                                                                            tokenizer=tokenizer)\n",
    "texts_pad, sentiment_scores, upvotes, tokenizer = prepare_score_data(comment_texts,\n",
    "                                                                     comment_sentiment_scores,\n",
    "                                                                     comment_upvotes,\n",
    "                                                                     150,\n",
    "                                                                     tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the processed data, the word -> index dictionary of th\n",
    "# tokenizer, and the tokenizer itself.\n",
    "np.save(\"parent_texts.npy\", pad_parents)\n",
    "np.save(\"parent_sentiment_scores.npy\", parent_scores)\n",
    "np.save(\"child_first_word.npy\", first_word)\n",
    "np.save(\"comment_texts.npy\", texts_pad)\n",
    "np.save(\"comment_sentiment_scores.npy\", sentiment_scores)\n",
    "np.save(\"comment_upvotes.npy\", upvotes)\n",
    "pickle.dump(tokenizer.word_index, open(\"words.pkl\", \"wb\"))\n",
    "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
